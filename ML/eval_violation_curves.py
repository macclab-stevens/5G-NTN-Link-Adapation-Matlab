#!/usr/bin/env python3
"""
Compute ROC and PR curves for violation detection using reports/predictions.csv
generated by evaluate_xgb.py, and save plots + metrics to reports/.
"""

import argparse
from pathlib import Path
import numpy as np
import polars as pl
import json
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


def compute_curves(y_true: np.ndarray, scores: np.ndarray):
    """Return ROC and PR data and AUC/AP.

    y_true: 1 = violation, 0 = pass
    scores: higher = more likely violation (e.g., 1 - prob_pass)
    """
    # Sort by score descending
    order = np.argsort(-scores)
    y = y_true[order].astype(np.int32)
    n = y.shape[0]
    pos = float(y.sum())
    neg = float(n - pos)

    # Cumulative counts along thresholds
    tp = np.cumsum(y)
    fp = np.cumsum(1 - y)
    fn = pos - tp
    tn = neg - fp

    # Avoid division by zero
    tpr = tp / np.maximum(pos, 1.0)
    fpr = fp / np.maximum(neg, 1.0)
    precision = tp / np.maximum((tp + fp), 1.0)
    recall = tpr

    # ROC AUC (trapezoidal over FPR)
    # Need to ensure starting at (0,0) and ending at (1,1)
    fpr_ext = np.concatenate(([0.0], fpr, [1.0]))
    tpr_ext = np.concatenate(([0.0], tpr, [1.0]))
    roc_auc = float(np.trapz(tpr_ext, fpr_ext))

    # PR curve; average precision via step-wise area: sum (Î”R) * P
    # Ensure (0, P0) and (1, P_end) coverage
    recall_ext = np.concatenate(([0.0], recall))
    precision_ext = np.concatenate(([precision[0] if precision.size > 0 else 1.0], precision))
    ap = 0.0
    for i in range(1, recall_ext.size):
        dR = recall_ext[i] - recall_ext[i - 1]
        ap += float(precision_ext[i] * dR)

    return {
        'roc': {'fpr': fpr.tolist(), 'tpr': tpr.tolist(), 'auc': roc_auc},
        'pr': {'recall': recall.tolist(), 'precision': precision.tolist(), 'ap': float(ap)},
        'threshold_scores_sorted': scores[order].tolist(),
    }


def main():
    ap = argparse.ArgumentParser(description="Compute ROC/PR curves for violation detection")
    ap.add_argument("--predictions", type=str, default="reports/predictions.csv", help="CSV from evaluate_xgb.py")
    ap.add_argument("--output-dir", type=str, default="reports", help="Where to save plots and metrics")
    args = ap.parse_args()

    out = Path(args.output_dir)
    out.mkdir(parents=True, exist_ok=True)

    df = pl.read_csv(args.predictions)
    if 'pred_prob' not in df.columns or 'label_pass' not in df.columns:
        raise SystemExit("predictions.csv must contain pred_prob and label_pass")

    y_pass = df['label_pass'].to_numpy().astype(np.int32)
    y_violation = 1 - y_pass
    scores = 1.0 - df['pred_prob'].to_numpy()

    curves = compute_curves(y_violation, scores)
    (out / 'violation_curves.json').write_text(json.dumps(curves, indent=2))

    # ROC plot
    plt.figure(figsize=(5, 4))
    plt.plot(curves['roc']['fpr'], curves['roc']['tpr'], label=f"ROC AUC={curves['roc']['auc']:.4f}")
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Violation ROC')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.savefig(out / 'violation_roc.png', dpi=160)
    plt.close()

    # PR plot
    plt.figure(figsize=(5, 4))
    plt.plot(curves['pr']['recall'], curves['pr']['precision'], label=f"AP={curves['pr']['ap']:.4f}")
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Violation Precision-Recall')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.savefig(out / 'violation_pr.png', dpi=160)
    plt.close()

    print(f"Saved ROC/PR curves and metrics to {out}")


if __name__ == "__main__":
    main()

